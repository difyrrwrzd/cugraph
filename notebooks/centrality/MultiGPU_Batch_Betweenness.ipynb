{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU Batch Betweenness Centrality\n",
    "In this notebook, we will compute the Betweenness Centrality for vertices using CuGraph and will see how to use multiple GPU to compute the Betweenness Centrality scores.\n",
    "\n",
    "RAPIDS Versions: 0.15\n",
    "\n",
    "Test Hardware:\n",
    "4 Tesla V100-DGX 32G, CUDA 10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Betweennes Centrality can be slow to compute on large graphs, in order to speed up the process we can leverage multiple GPUs.\n",
    "In this notebook we will showcase how it would have been done with a Single GPU approach, then we will show how it can be done using multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cugraph\n",
    "import cudf\n",
    "\n",
    "import dask\n",
    "import dask_cuda\n",
    "import dask_cudf\n",
    "import cugraph.comms as Comms\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile='../data/csv/directed/soc-LiveJournal1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data - Single GPU\n",
    "The following shows how we would read the csv file using a single GPU as it commonly done when using a single GPU with CuGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_read_sg = time.perf_counter()\n",
    "e_list = cudf.read_csv(datafile, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])\n",
    "t_stop_read_sg = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG Read time: 1.4486296999966726s\n"
     ]
    }
   ],
   "source": [
    "print(\"SG Read time: {}s\".format(t_stop_read_sg - t_start_read_sg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Graph - Single GPU\n",
    "Once we read the file, we need to build the Graph, we will use a DiGraph, and use the content extracted from the .csv file as an edge list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_build_sg = time.perf_counter()\n",
    "G = cugraph.DiGraph()\n",
    "G.from_cudf_edgelist(e_list, source='src', destination='dst')\n",
    "t_stop_build_sg = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG Build time: 0.8520178709877655s\n"
     ]
    }
   ],
   "source": [
    "print(\"SG Build time: {}s\".format(t_stop_build_sg - t_start_build_sg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the Algorithm -  Single GPU\n",
    "Now that our graph is built, we can get its betweenness centrality score. Here we will use a sub-sample of 1024 sources in order to have a better approximation of the overall betweenness centrality. We set the set for comparability with the multi GPU version that comes next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_sg = time.perf_counter()\n",
    "sg_df = cugraph.betweenness_centrality(G, k=1024, seed=123)\n",
    "t_stop_sg = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG Time elapsed: 57.219933932996355s\n"
     ]
    }
   ],
   "source": [
    "print(\"SG Time elapsed: {}s\".format(t_stop_sg - t_start_sg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's use multiple GPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single chunk\n",
    "In order to use multi GPU Batch Betweenness Centrality, we need to ensure that the data is represented as a single chunk on a single GPU.\n",
    "The following function gives us the size required to fit the entire data in a single chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunksize(input_file):\n",
    "    return os.path.getsize(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Dask Cluster\n",
    "In order to use multiple GPU, we need to ensure that we have Dask Cluster and Client running, further more we need to initialize the CuGraph Communicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_cuda.LocalCUDACluster()\n",
    "client = dask.distributed.Client(cluster)\n",
    "Comms.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data - Multiple GPUs\n",
    "This step is quite similar to the single GPU version:\n",
    "1. We need to get the `chunksize`\n",
    "2. We call `dask_cudf.read_csv` instead of `cudf.read_csv`, and use `chunksize=chunksize` to ensure that the data is in a single partition.\n",
    "As you can see the difference are quite minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_read_mg = time.perf_counter()\n",
    "chunksize = get_chunksize(datafile)\n",
    "dask_e_list = dask_cudf.read_csv(datafile, chunksize=chunksize, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])\n",
    "t_stop_read_mg = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG Read time: 0.02092634595464915s\n"
     ]
    }
   ],
   "source": [
    "print(\"MG Read time: {}s\".format(t_stop_read_mg - t_start_read_mg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Graph - Mutliple GPUs\n",
    "Once we read the file, we need to build the Graph, we will use a DiGraph, and use the content extracted from the .csv file as an edge list, this comes with some overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_build_mg = time.perf_counter()\n",
    "RG = cugraph.DiGraph()\n",
    "RG.from_dask_cudf_edgelist(dask_e_list)\n",
    "t_stop_build_mg = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG Build time: 7.060216261015739s\n"
     ]
    }
   ],
   "source": [
    "print(\"MG Build time: {}s\".format(t_stop_build_mg - t_start_build_mg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the algorithm\n",
    "We call the algorithm the same way as we used to, but this time it is much faster as we leverage multiple GPUs to compute the Betweenness Centrality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcadet/miniconda3/envs/cugraph_dev/lib/python3.7/site-packages/distributed/client.py:3493: RuntimeWarning: coroutine 'Client._update_scheduler_info' was never awaited\n",
      "  self.sync(self._update_scheduler_info)\n"
     ]
    }
   ],
   "source": [
    "t_start_mg = time.perf_counter()\n",
    "mg_df = cugraph.betweenness_centrality(RG, k=1024, seed=123)\n",
    "t_stop_mg = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG Time elapsed: 15.80565904499963s\n"
     ]
    }
   ],
   "source": [
    "print(\"MG Time elapsed: {}s\".format(t_stop_mg - t_start_mg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to clear the Communicator / client /cluster if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comms.destroy()\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
