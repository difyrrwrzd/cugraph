{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU Batch Betweenness Centrality\n",
    "In this notebook, we will compute the Betweenness Centrality for vertices using CuGraph and will see how to use multiple GPU to compute the Betweenness Centrality scores.\n",
    "\n",
    "RAPIDS Versions: 0.15\n",
    "\n",
    "Test Hardware:\n",
    "4 Tesla V100-DGX 32G, CUDA 10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Betweennes Centrality can be slow to compute on large graphs, in order to speed up the process we can leverage multiple GPUs.\n",
    "In this notebook we will showcase how it would have been done with a Single GPU approach, then we will show how it can be done using multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cugraph\n",
    "import cudf\n",
    "\n",
    "import dask\n",
    "import dask_cuda\n",
    "import dask_cudf\n",
    "import cugraph.comms as Comms\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Automate download\n",
    "datafile='../data/csv/directed/soc-LiveJournal1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data - Single GPU\n",
    "The following shows how we would read the csv file using a single GPU as it commonly done when using a single GPU with CuGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_read_sg = time.perf_counter()\n",
    "e_list = cudf.read_csv(datafile, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])\n",
    "t_stop_read_sg = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG Read time: 1.5347624099813402s\n"
     ]
    }
   ],
   "source": [
    "print(\"SG Read time: {}s\".format(t_stop_read_sg - t_start_read_sg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Graph - Single GPU\n",
    "Once we read the file, we need to build the Graph, we will use a DiGraph, and use the content extracted from the .csv file as an edge list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_build_sg = time.perf_counter()\n",
    "G = cugraph.DiGraph()\n",
    "G.from_cudf_edgelist(e_list, source='src', destination='dst')\n",
    "t_stop_build_sg = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG Build time: 0.8592527429573238s\n"
     ]
    }
   ],
   "source": [
    "print(\"SG Build time: {}s\".format(t_stop_build_sg - t_start_build_sg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the Algorithm -  Single GPU\n",
    "Now that our graph is built, we can get its betweenness centrality score. Here we will use a sub-sample of 1024 sources in order to have a better approximation of the overall betweenness centrality. We set the set for comparability with the multi GPU version that comes next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_sg = time.perf_counter()\n",
    "sg_df = cugraph.betweenness_centrality(G, k=1024, seed=123)\n",
    "t_stop_sg = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG Time elapsed: 57.58988218102604s\n"
     ]
    }
   ],
   "source": [
    "print(\"SG Time elapsed: {}s\".format(t_stop_sg - t_start_sg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's use multiple GPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Dask Cluster\n",
    "In order to use multiple GPU, we need to ensure that we have Dask Cluster and Client running, further more we need to initialize the CuGraph Communicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_cuda.LocalCUDACluster()\n",
    "client = dask.distributed.Client(cluster)\n",
    "Comms.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling Multi GPU Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DBG] Enabled Multi-GPU Batch\n",
      "MG Batch Enabling Time elapsed: 0.007048942963592708s\n"
     ]
    }
   ],
   "source": [
    "t_start_mg = time.perf_counter()\n",
    "G.enable_mg_batch()\n",
    "print(\"MG Batch Enabling Time elapsed: {}s\".format(time.perf_counter() - t_start_mg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the algorithm\n",
    "We call the algorithm the same way as we used to, but this time it is much faster as we leverage multiple GPUs to compute the Betweenness Centrality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcadet/miniconda3/envs/cugraph_dev/lib/python3.7/site-packages/distributed/client.py:3493: RuntimeWarning: coroutine 'Client._update_scheduler_info' was never awaited\n",
      "  self.sync(self._update_scheduler_info)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG call:  15.899459821986966\n"
     ]
    }
   ],
   "source": [
    "t_start_mg = time.perf_counter()\n",
    "batch_df = cugraph.betweenness_centrality(G, k=1024, seed=123)\n",
    "t_stop_mg = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG Time elapsed: 15.96886160003487s\n"
     ]
    }
   ],
   "source": [
    "print(\"MG Time elapsed: {}s\".format(t_stop_mg - t_start_mg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to clear the Communicator / client /cluster if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comms.destroy()\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
